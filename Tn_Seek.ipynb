{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNs7S5HDxeJh4XGg2nGN7NW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MishterBluesky/Tn-seek/blob/master/Tn_Seek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tn-Seek - Automated Tn-seq processing and analysis\n",
        "\n",
        "This software uses custom scripts to analyse tn-seq data. Simply select your genome and upload your fastq files. It will detect common sequences and remove the inverted repeats to clean your data automatically.\n",
        "\n",
        "The comparison of insertion will be automatically processed for you into a csv file."
      ],
      "metadata": {
        "id": "wLRQEiNZV18D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download Tn-Seek software and genome\n",
        "#@markdown please provide a direct url to your genome files for download.\n",
        "!pip install biopython pandas matplotlib\n",
        "!wget -O miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x miniconda.sh\n",
        "!bash ./miniconda.sh -b -f -p /usr/local\n",
        "\n",
        "# Update PATH environment variable to include conda\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages')\n",
        "!conda init bash\n",
        "\n",
        "# Install bowtie2\n",
        "!apt-get install -y bowtie2\n",
        "\n",
        "# Create a directory for the reference genome\n",
        "import os\n",
        "os.makedirs('/content/ref_genome/GENOME', exist_ok=True)\n",
        "\n",
        "# Download the correct reference genome file\n",
        "url = 'https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/009/045/GCF_000009045.1_ASM904v1/GCF_000009045.1_ASM904v1_genomic.fna.gz' #@param\n",
        "correct_url = url\n",
        "!wget -P /content/ref_genome/GENOME {correct_url}\n",
        "\n",
        "# Unzip the genome file if it's in .gz format\n",
        "!gunzip /content/ref_genome/GENOME/GCF_000009045.1_ASM904v1_genomic.fna.gz\n",
        "\n",
        "# Check if the genome file is downloaded correctly\n",
        "genome_path = '/content/ref_genome/GENOME/GCF_000009045.1_ASM904v1_genomic.fna'\n",
        "if os.path.exists(genome_path):\n",
        "    print(\"Genome file downloaded successfully.\")\n",
        "else:\n",
        "    print(\"Error: Genome file not found.\")\n",
        "    # Exit if the genome file is not found\n",
        "    exit(1)\n",
        "\n",
        "# Clone the GitHub repository\n",
        "!git clone https://github.com/MishterBluesky/Tn-seek.git\n",
        "\n",
        "# Navigate to the script directory\n",
        "%cd Tn-seek\n",
        "\n",
        "# Make the script executable\n",
        "!chmod +x TnSeq3-2.sh"
      ],
      "metadata": {
        "id": "W5jumF68PLhn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "120a602c-7ad9-4fd2-b3f7-763ec807a20a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting biopython\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Installing collected packages: biopython\n",
            "Successfully installed biopython-1.84\n",
            "--2024-07-16 09:21:02--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.32.241, 104.16.191.158, 2606:4700::6810:bf9e, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.32.241|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 146836934 (140M) [application/octet-stream]\n",
            "Saving to: ‘miniconda.sh’\n",
            "\n",
            "miniconda.sh        100%[===================>] 140.03M   235MB/s    in 0.6s    \n",
            "\n",
            "2024-07-16 09:21:03 (235 MB/s) - ‘miniconda.sh’ saved [146836934/146836934]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "\n",
            "Installing base environment...\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "no change     /usr/local/condabin/conda\n",
            "no change     /usr/local/bin/conda\n",
            "no change     /usr/local/bin/conda-env\n",
            "no change     /usr/local/bin/activate\n",
            "no change     /usr/local/bin/deactivate\n",
            "no change     /usr/local/etc/profile.d/conda.sh\n",
            "no change     /usr/local/etc/fish/conf.d/conda.fish\n",
            "no change     /usr/local/shell/condabin/Conda.psm1\n",
            "no change     /usr/local/shell/condabin/conda-hook.ps1\n",
            "no change     /usr/local/lib/python3.12/site-packages/xontrib/conda.xsh\n",
            "no change     /usr/local/etc/profile.d/conda.csh\n",
            "modified      /root/.bashrc\n",
            "\n",
            "==> For changes to take effect, close and re-open your current shell. <==\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  bowtie2-examples\n",
            "The following NEW packages will be installed:\n",
            "  bowtie2\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,169 kB of archives.\n",
            "After this operation, 3,735 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 bowtie2 amd64 2.4.4-1 [1,169 kB]\n",
            "Fetched 1,169 kB in 2s (557 kB/s)\n",
            "Selecting previously unselected package bowtie2.\n",
            "(Reading database ... 123576 files and directories currently installed.)\n",
            "Preparing to unpack .../bowtie2_2.4.4-1_amd64.deb ...\n",
            "Unpacking bowtie2 (2.4.4-1) ...\n",
            "Setting up bowtie2 (2.4.4-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "--2024-07-16 09:21:36--  https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/009/045/GCF_000009045.1_ASM904v1/GCF_000009045.1_ASM904v1_genomic.fna.gz\n",
            "Resolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 130.14.250.13, 130.14.250.10, 130.14.250.11, ...\n",
            "Connecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|130.14.250.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1248396 (1.2M) [application/x-gzip]\n",
            "Saving to: ‘/content/ref_genome/GENOME/GCF_000009045.1_ASM904v1_genomic.fna.gz’\n",
            "\n",
            "GCF_000009045.1_ASM 100%[===================>]   1.19M   986KB/s    in 1.2s    \n",
            "\n",
            "2024-07-16 09:21:38 (986 KB/s) - ‘/content/ref_genome/GENOME/GCF_000009045.1_ASM904v1_genomic.fna.gz’ saved [1248396/1248396]\n",
            "\n",
            "Genome file downloaded successfully.\n",
            "Cloning into 'Tn-seek'...\n",
            "remote: Enumerating objects: 267, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 267 (delta 31), reused 0 (delta 0), pack-reused 209\u001b[K\n",
            "Receiving objects: 100% (267/267), 99.80 KiB | 973.00 KiB/s, done.\n",
            "Resolving deltas: 100% (151/151), done.\n",
            "/content/Tn-seek\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/marcelm/cutadapt.git\n",
        "!pip install ./cutadapt\n",
        "!cutadapt --version"
      ],
      "metadata": {
        "id": "M9FH4K00Z4xU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1ca1e1-7c34-4260-a6c8-484cb80746e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cutadapt'...\n",
            "remote: Enumerating objects: 12896, done.\u001b[K\n",
            "remote: Counting objects: 100% (2505/2505), done.\u001b[K\n",
            "remote: Compressing objects: 100% (655/655), done.\u001b[K\n",
            "remote: Total 12896 (delta 1647), reused 2416 (delta 1578), pack-reused 10391\u001b[K\n",
            "Receiving objects: 100% (12896/12896), 3.48 MiB | 8.69 MiB/s, done.\n",
            "Resolving deltas: 100% (8428/8428), done.\n",
            "Processing ./cutadapt\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dnaio>=1.2.0 (from cutadapt==4.10.dev2+gadfbc18)\n",
            "  Downloading dnaio-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting xopen>=1.6.0 (from cutadapt==4.10.dev2+gadfbc18)\n",
            "  Downloading xopen-2.0.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting isal>=1.6.1 (from xopen>=1.6.0->cutadapt==4.10.dev2+gadfbc18)\n",
            "  Downloading isal-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Collecting zlib-ng>=0.4.1 (from xopen>=1.6.0->cutadapt==4.10.dev2+gadfbc18)\n",
            "  Downloading zlib_ng-0.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Downloading dnaio-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.9/109.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xopen-2.0.2-py3-none-any.whl (17 kB)\n",
            "Downloading isal-1.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (258 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.6/258.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zlib_ng-0.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: cutadapt\n",
            "  Building wheel for cutadapt (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cutadapt: filename=cutadapt-4.10.dev2+gadfbc18-cp312-cp312-linux_x86_64.whl size=266749 sha256=ce7df224daedb3307eb10911d465f3e5e3ed2e1b39744bf3ee4eeb03584be41f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-hcy0oayv/wheels/0c/38/0b/9d76e87e12c1e562ab8ced2e28340519d649faa5d8c2c2b359\n",
            "Successfully built cutadapt\n",
            "Installing collected packages: zlib-ng, isal, xopen, dnaio, cutadapt\n",
            "Successfully installed cutadapt-4.10.dev2+gadfbc18 dnaio-1.2.1 isal-1.6.1 xopen-2.0.2 zlib-ng-0.4.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m4.10.dev2+gadfbc18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "!cd /content/Tn-seek/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dr7aKindSmKK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wildtype analysis\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eGI62icfXRrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/Tn-seek/\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "#@markdown Please upload your wildtype control fastq file. This takes some time typically.\n",
        "original_name = next(iter(uploaded.keys()))\n",
        "# New file name (change extension to '.fastq')\n",
        "new_name = original_name.replace('.fq', '.fastq')\n",
        "\n",
        "# Save the uploaded file with the new name\n",
        "with open(new_name, 'wb') as f:\n",
        "    f.write(uploaded[original_name])\n",
        "genome_path1 = f'/content/ref_genome/GENOME/GENOME'\n",
        "wildtype_filename = original_name.replace('.fq', '')\n",
        "\n",
        "prefix1 = wildtype_filename"
      ],
      "metadata": {
        "id": "Dly7amp9POxI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "48469220-8331-437c-c792-76295425d1f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-574af31b-45fd-43ba-a27b-b344f8173e80\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-574af31b-45fd-43ba-a27b-b344f8173e80\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2413.fastq to 2413.fastq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Optionally we will save your fastq file to your google drive if it isnt already, as uploading this takes a long time.\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "filename = f'{wildtype_filename}.fastq'\n",
        "destination_folder = '/content/drive/My Drive/Tn-seek/'\n",
        "for filename in uploaded.keys():\n",
        "    shutil.copy(filename, destination_folder + filename)\n",
        "    print(f'copied \"{filename}\" to \"{destination_folder}\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7Cee7fKZ5Rd",
        "outputId": "7c823f6b-c7eb-485a-d71c-a5a8acb527ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "copied \"2413.fastq\" to \"/content/drive/My Drive/Tn-seek/\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown This code will find repeat sequences within your fastq and remove them..\n",
        "# Step 1: Read the File and Sample the First 1000 Lines\n",
        "sample_size = 10000\n",
        "wildtype_filename = wildtype_filename.replace('.fastq', '')\n",
        "prefix1= wildtype_filename\n",
        "file_path = filename\n",
        "print(file_path)\n",
        "lines = []\n",
        "with open(file_path, 'r') as file:\n",
        "    for i in range(sample_size):\n",
        "        line = file.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        lines.append(line.strip())\n",
        "\n",
        "# Step 2: Extract Nucleotide Sequences\n",
        "nucleotide_sequences = [lines[i] for i in range(1, len(lines), 4)]\n",
        "\n",
        "# Step 3: Count Occurrences of Sequences\n",
        "from collections import defaultdict\n",
        "\n",
        "sequence_counts = defaultdict(int)\n",
        "\n",
        "def count_subsequences(seq, min_length):\n",
        "    for length in range(min_length, len(seq) + 1):\n",
        "        for i in range(len(seq) - length + 1):\n",
        "            subseq = seq[i:i+length]\n",
        "            sequence_counts[subseq] += 1\n",
        "\n",
        "# Count subsequences for all nucleotide sequences\n",
        "for seq in nucleotide_sequences:\n",
        "    count_subsequences(seq, 5)\n",
        "\n",
        "total_sequences = len(nucleotide_sequences)\n",
        "\n",
        "# Step 4: Determine Frequent Sequences\n",
        "frequent_sequences = {\n",
        "    seq: count for seq, count in sequence_counts.items()\n",
        "    if (len(seq) > 15 and count / total_sequences > 0.10) or\n",
        "       (len(seq) > 7 and count / total_sequences > 0.30)\n",
        "}\n",
        "\n",
        "# Output the results\n",
        "frequent_sequences_list = list(frequent_sequences.keys())\n",
        "output_filename = f'IR_sequences_{wildtype_filename}.txt'\n",
        "with open(output_filename, 'w') as file:\n",
        "    for seq in frequent_sequences_list:\n",
        "        file.write(f\"{seq}\\n\")\n",
        "# Read the sequences from the file into a list\n",
        "with open(output_filename, 'r') as file:\n",
        "    sequences = [line.strip() for line in file]\n",
        "\n",
        "# Sort sequences based on their lengths in descending order\n",
        "sequences_sorted = sorted(sequences, key=len, reverse=True)\n",
        "\n",
        "# Write the sorted sequences back to the file\n",
        "with open(f'IR_sequences_{wildtype_filename}_sorted.txt', 'w') as file:\n",
        "    for sequence in sequences_sorted:\n",
        "        file.write(sequence + '\\n')\n",
        "print(f'Saved sequences to IR_sequences_{wildtype_filename}_sorted.txt')\n",
        "\n",
        "output_filename = f'IR_sequences_{wildtype_filename}_sorted.txt'\n",
        "with open(output_filename, 'r') as file:\n",
        "    sequences = [line.strip() for line in file]\n",
        "\n",
        "# Step 2: Sort sequences based on their lengths in descending order\n",
        "sequences_sorted = sorted(sequences, key=len, reverse=True)\n",
        "\n",
        "# Step 3: Write the sorted sequences to a new file in numbered FASTA format\n",
        "fasta_output_filename = f'IR_sequences_{wildtype_filename}_sorted.fasta'\n",
        "with open(fasta_output_filename, 'w') as file:\n",
        "    for index, sequence in enumerate(sequences_sorted, start=1):\n",
        "        file.write(f\">Sequence_{index}\\n\")\n",
        "        file.write(f\"{sequence}\\n\")\n",
        "\n",
        "print(f'Saved sequences to {fasta_output_filename}')"
      ],
      "metadata": {
        "id": "nXbZuFSn1jFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "687f2e74-fbc5-401f-8c5d-3853905fb40a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2413.fastq\n",
            "Saved sequences to IR_sequences_2413_sorted.txt\n",
            "Saved sequences to IR_sequences_2413_sorted.fasta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "fasta = f'IR_sequences_{wildtype_filename}_sorted.fasta'\n",
        "# Copy original fastq file to .trim.fastq\n",
        "shutil.copyfile(f'{wildtype_filename}.fastq', f'{wildtype_filename}.trim.fastq')\n",
        "fastq_file = f'{wildtype_filename}.fastq'\n",
        "cut_fastq = f'{wildtype_filename}.trim.fastq'\n",
        "# Iterate over sequences and run cutadapt\n",
        "!cutadapt -b file:{fasta} -m 15 -l 15 -o {cut_fastq} {fastq_file}\n",
        "print(\"All sequences processed with cutadapt.\")"
      ],
      "metadata": {
        "id": "nPMysJv--Pex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684f3baf-944c-437a-f313-1412f5ffdb90"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is cutadapt 4.10.dev2+gadfbc18 with Python 3.12.4\n",
            "Command line parameters: -b file:IR_sequences_2413_sorted.fasta -m 15 -l 15 -o 2413.trim.fastq 2413.fastq\n",
            "Processing single-end reads on 1 core ...\n",
            "Done           00:00:26     8,476,106 reads @   3.1 µs/read;  19.08 M reads/minute\n",
            "Finished in 26.653 s (3.145 µs/read; 19.08 M reads/minute).\n",
            "\n",
            "=== Summary ===\n",
            "\n",
            "Total reads processed:               8,476,106\n",
            "\n",
            "== Read fate breakdown ==\n",
            "Reads that were too short:                   0 (0.0%)\n",
            "Reads written (passing filters):     8,476,106 (100.0%)\n",
            "\n",
            "Total basepairs processed:   135,617,696 bp\n",
            "Total written (filtered):    127,141,590 bp (93.8%)\n",
            "All sequences processed with cutadapt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefix1= wildtype_filename\n",
        "genome_path = '/content/ref_genome/GENOME/GCF_000009045.1_ASM904v1_genomic.fna'\n",
        "!bowtie2-build {genome_path} /content/ref_genome/GENOME/GENOME\n",
        "ir_seq = 'GCTA'\n",
        "!./TnSeq3-2.sh -i {ir_seq} -g /content/ref_genome/GENOME/GENOME {prefix1}"
      ],
      "metadata": {
        "id": "UYU-zvkzwZmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4f2c1e-213a-4b31-9172-60d85c9c22b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings:\n",
            "  Output files: \"/content/ref_genome/GENOME/GENOME.*.bt2\"\n",
            "  Line rate: 6 (line is 64 bytes)\n",
            "  Lines per side: 1 (side is 64 bytes)\n",
            "  Offset rate: 4 (one in 16)\n",
            "  FTable chars: 10\n",
            "  Strings: unpacked\n",
            "  Max bucket size: default\n",
            "  Max bucket size, sqrt multiplier: default\n",
            "  Max bucket size, len divisor: 4\n",
            "  Difference-cover sample period: 1024\n",
            "  Endianness: little\n",
            "  Actual local endianness: little\n",
            "  Sanity checking: disabled\n",
            "  Assertions: disabled\n",
            "  Random seed: 0\n",
            "  Sizeofs: void*:8, int:4, long:8, size_t:8\n",
            "Input files DNA, FASTA:\n",
            "  /content/ref_genome/GENOME/GCF_000009045.1_ASM904v1_genomic.fna\n",
            "Building a SMALL index\n",
            "Reading reference sizes\n",
            "  Time reading reference sizes: 00:00:00\n",
            "Calculating joined length\n",
            "Writing header\n",
            "Reserving space for joined string\n",
            "Joining reference sequences\n",
            "  Time to join reference sequences: 00:00:00\n",
            "bmax according to bmaxDivN setting: 1053901\n",
            "Using parameters --bmax 790426 --dcv 1024\n",
            "  Doing ahead-of-time memory usage test\n",
            "  Passed!  Constructing with these parameters: --bmax 790426 --dcv 1024\n",
            "Constructing suffix-array element generator\n",
            "Building DifferenceCoverSample\n",
            "  Building sPrime\n",
            "  Building sPrimeOrder\n",
            "  V-Sorting samples\n",
            "  V-Sorting samples time: 00:00:00\n",
            "  Allocating rank array\n",
            "  Ranking v-sort output\n",
            "  Ranking v-sort output time: 00:00:00\n",
            "  Invoking Larsson-Sadakane on ranks\n",
            "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
            "  Sanity-checking and returning\n",
            "Building samples\n",
            "Reserving space for 12 sample suffixes\n",
            "Generating random suffixes\n",
            "QSorting 12 sample offsets, eliminating duplicates\n",
            "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
            "Multikey QSorting 12 samples\n",
            "  (Using difference cover)\n",
            "  Multikey QSorting samples time: 00:00:00\n",
            "Calculating bucket sizes\n",
            "Splitting and merging\n",
            "  Splitting and merging time: 00:00:00\n",
            "Split 1, merged 5; iterating...\n",
            "Splitting and merging\n",
            "  Splitting and merging time: 00:00:01\n",
            "Avg bucket size: 468400 (target: 790425)\n",
            "Converting suffix-array elements to index image\n",
            "Allocating ftab, absorbFtab\n",
            "Entering Ebwt loop\n",
            "Getting block 1 of 9\n",
            "  Reserving size (790426) for bucket 1\n",
            "  Calculating Z arrays for bucket 1\n",
            "  Entering block accumulator loop for bucket 1:\n",
            "  bucket 1: 10%\n",
            "  bucket 1: 20%\n",
            "  bucket 1: 30%\n",
            "  bucket 1: 40%\n",
            "  bucket 1: 50%\n",
            "  bucket 1: 60%\n",
            "  bucket 1: 70%\n",
            "  bucket 1: 80%\n",
            "  bucket 1: 90%\n",
            "  bucket 1: 100%\n",
            "  Sorting block of length 411522 for bucket 1\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 411523 for bucket 1\n",
            "Getting block 2 of 9\n",
            "  Reserving size (790426) for bucket 2\n",
            "  Calculating Z arrays for bucket 2\n",
            "  Entering block accumulator loop for bucket 2:\n",
            "  bucket 2: 10%\n",
            "  bucket 2: 20%\n",
            "  bucket 2: 30%\n",
            "  bucket 2: 40%\n",
            "  bucket 2: 50%\n",
            "  bucket 2: 60%\n",
            "  bucket 2: 70%\n",
            "  bucket 2: 80%\n",
            "  bucket 2: 90%\n",
            "  bucket 2: 100%\n",
            "  Sorting block of length 545597 for bucket 2\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 545598 for bucket 2\n",
            "Getting block 3 of 9\n",
            "  Reserving size (790426) for bucket 3\n",
            "  Calculating Z arrays for bucket 3\n",
            "  Entering block accumulator loop for bucket 3:\n",
            "  bucket 3: 10%\n",
            "  bucket 3: 20%\n",
            "  bucket 3: 30%\n",
            "  bucket 3: 40%\n",
            "  bucket 3: 50%\n",
            "  bucket 3: 60%\n",
            "  bucket 3: 70%\n",
            "  bucket 3: 80%\n",
            "  bucket 3: 90%\n",
            "  bucket 3: 100%\n",
            "  Sorting block of length 442795 for bucket 3\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 442796 for bucket 3\n",
            "Getting block 4 of 9\n",
            "  Reserving size (790426) for bucket 4\n",
            "  Calculating Z arrays for bucket 4\n",
            "  Entering block accumulator loop for bucket 4:\n",
            "  bucket 4: 10%\n",
            "  bucket 4: 20%\n",
            "  bucket 4: 30%\n",
            "  bucket 4: 40%\n",
            "  bucket 4: 50%\n",
            "  bucket 4: 60%\n",
            "  bucket 4: 70%\n",
            "  bucket 4: 80%\n",
            "  bucket 4: 90%\n",
            "  bucket 4: 100%\n",
            "  Sorting block of length 547118 for bucket 4\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 547119 for bucket 4\n",
            "Getting block 5 of 9\n",
            "  Reserving size (790426) for bucket 5\n",
            "  Calculating Z arrays for bucket 5\n",
            "  Entering block accumulator loop for bucket 5:\n",
            "  bucket 5: 10%\n",
            "  bucket 5: 20%\n",
            "  bucket 5: 30%\n",
            "  bucket 5: 40%\n",
            "  bucket 5: 50%\n",
            "  bucket 5: 60%\n",
            "  bucket 5: 70%\n",
            "  bucket 5: 80%\n",
            "  bucket 5: 90%\n",
            "  bucket 5: 100%\n",
            "  Sorting block of length 662679 for bucket 5\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:01\n",
            "Returning block of 662680 for bucket 5\n",
            "Getting block 6 of 9\n",
            "  Reserving size (790426) for bucket 6\n",
            "  Calculating Z arrays for bucket 6\n",
            "  Entering block accumulator loop for bucket 6:\n",
            "  bucket 6: 10%\n",
            "  bucket 6: 20%\n",
            "  bucket 6: 30%\n",
            "  bucket 6: 40%\n",
            "  bucket 6: 50%\n",
            "  bucket 6: 60%\n",
            "  bucket 6: 70%\n",
            "  bucket 6: 80%\n",
            "  bucket 6: 90%\n",
            "  bucket 6: 100%\n",
            "  Sorting block of length 417108 for bucket 6\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 417109 for bucket 6\n",
            "Getting block 7 of 9\n",
            "  Reserving size (790426) for bucket 7\n",
            "  Calculating Z arrays for bucket 7\n",
            "  Entering block accumulator loop for bucket 7:\n",
            "  bucket 7: 10%\n",
            "  bucket 7: 20%\n",
            "  bucket 7: 30%\n",
            "  bucket 7: 40%\n",
            "  bucket 7: 50%\n",
            "  bucket 7: 60%\n",
            "  bucket 7: 70%\n",
            "  bucket 7: 80%\n",
            "  bucket 7: 90%\n",
            "  bucket 7: 100%\n",
            "  Sorting block of length 378276 for bucket 7\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 378277 for bucket 7\n",
            "Getting block 8 of 9\n",
            "  Reserving size (790426) for bucket 8\n",
            "  Calculating Z arrays for bucket 8\n",
            "  Entering block accumulator loop for bucket 8:\n",
            "  bucket 8: 10%\n",
            "  bucket 8: 20%\n",
            "  bucket 8: 30%\n",
            "  bucket 8: 40%\n",
            "  bucket 8: 50%\n",
            "  bucket 8: 60%\n",
            "  bucket 8: 70%\n",
            "  bucket 8: 80%\n",
            "  bucket 8: 90%\n",
            "  bucket 8: 100%\n",
            "  Sorting block of length 771113 for bucket 8\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 771114 for bucket 8\n",
            "Getting block 9 of 9\n",
            "  Reserving size (790426) for bucket 9\n",
            "  Calculating Z arrays for bucket 9\n",
            "  Entering block accumulator loop for bucket 9:\n",
            "  bucket 9: 10%\n",
            "  bucket 9: 20%\n",
            "  bucket 9: 30%\n",
            "  bucket 9: 40%\n",
            "  bucket 9: 50%\n",
            "  bucket 9: 60%\n",
            "  bucket 9: 70%\n",
            "  bucket 9: 80%\n",
            "  bucket 9: 90%\n",
            "  bucket 9: 100%\n",
            "  Sorting block of length 39390 for bucket 9\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 39391 for bucket 9\n",
            "Exited Ebwt loop\n",
            "fchr[A]: 0\n",
            "fchr[C]: 1188073\n",
            "fchr[G]: 2107357\n",
            "fchr[T]: 3022469\n",
            "fchr[$]: 4215606\n",
            "Exiting Ebwt::buildToDisk()\n",
            "Returning from initFromVector\n",
            "Wrote 5599784 bytes to primary EBWT file: /content/ref_genome/GENOME/GENOME.1.bt2\n",
            "Wrote 1053908 bytes to secondary EBWT file: /content/ref_genome/GENOME/GENOME.2.bt2\n",
            "Re-opening _in1 and _in2 as input streams\n",
            "Returning from Ebwt constructor\n",
            "Headers:\n",
            "    len: 4215606\n",
            "    bwtLen: 4215607\n",
            "    sz: 1053902\n",
            "    bwtSz: 1053902\n",
            "    lineRate: 6\n",
            "    offRate: 4\n",
            "    offMask: 0xfffffff0\n",
            "    ftabChars: 10\n",
            "    eftabLen: 20\n",
            "    eftabSz: 80\n",
            "    ftabLen: 1048577\n",
            "    ftabSz: 4194308\n",
            "    offsLen: 263476\n",
            "    offsSz: 1053904\n",
            "    lineSz: 64\n",
            "    sideSz: 64\n",
            "    sideBwtSz: 48\n",
            "    sideBwtLen: 192\n",
            "    numSides: 21957\n",
            "    numLines: 21957\n",
            "    ebwtTotLen: 1405248\n",
            "    ebwtTotSz: 1405248\n",
            "    color: 0\n",
            "    reverse: 0\n",
            "Total time for call to driver() for forward index: 00:00:02\n",
            "Reading reference sizes\n",
            "  Time reading reference sizes: 00:00:00\n",
            "Calculating joined length\n",
            "Writing header\n",
            "Reserving space for joined string\n",
            "Joining reference sequences\n",
            "  Time to join reference sequences: 00:00:00\n",
            "  Time to reverse reference sequence: 00:00:00\n",
            "bmax according to bmaxDivN setting: 1053901\n",
            "Using parameters --bmax 790426 --dcv 1024\n",
            "  Doing ahead-of-time memory usage test\n",
            "  Passed!  Constructing with these parameters: --bmax 790426 --dcv 1024\n",
            "Constructing suffix-array element generator\n",
            "Building DifferenceCoverSample\n",
            "  Building sPrime\n",
            "  Building sPrimeOrder\n",
            "  V-Sorting samples\n",
            "  V-Sorting samples time: 00:00:00\n",
            "  Allocating rank array\n",
            "  Ranking v-sort output\n",
            "  Ranking v-sort output time: 00:00:00\n",
            "  Invoking Larsson-Sadakane on ranks\n",
            "  Invoking Larsson-Sadakane on ranks time: 00:00:00\n",
            "  Sanity-checking and returning\n",
            "Building samples\n",
            "Reserving space for 12 sample suffixes\n",
            "Generating random suffixes\n",
            "QSorting 12 sample offsets, eliminating duplicates\n",
            "QSorting sample offsets, eliminating duplicates time: 00:00:00\n",
            "Multikey QSorting 12 samples\n",
            "  (Using difference cover)\n",
            "  Multikey QSorting samples time: 00:00:00\n",
            "Calculating bucket sizes\n",
            "Splitting and merging\n",
            "  Splitting and merging time: 00:00:00\n",
            "Split 1, merged 6; iterating...\n",
            "Splitting and merging\n",
            "  Splitting and merging time: 00:00:00\n",
            "Avg bucket size: 526950 (target: 790425)\n",
            "Converting suffix-array elements to index image\n",
            "Allocating ftab, absorbFtab\n",
            "Entering Ebwt loop\n",
            "Getting block 1 of 8\n",
            "  Reserving size (790426) for bucket 1\n",
            "  Calculating Z arrays for bucket 1\n",
            "  Entering block accumulator loop for bucket 1:\n",
            "  bucket 1: 10%\n",
            "  bucket 1: 20%\n",
            "  bucket 1: 30%\n",
            "  bucket 1: 40%\n",
            "  bucket 1: 50%\n",
            "  bucket 1: 60%\n",
            "  bucket 1: 70%\n",
            "  bucket 1: 80%\n",
            "  bucket 1: 90%\n",
            "  bucket 1: 100%\n",
            "  Sorting block of length 503634 for bucket 1\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 503635 for bucket 1\n",
            "Getting block 2 of 8\n",
            "  Reserving size (790426) for bucket 2\n",
            "  Calculating Z arrays for bucket 2\n",
            "  Entering block accumulator loop for bucket 2:\n",
            "  bucket 2: 10%\n",
            "  bucket 2: 20%\n",
            "  bucket 2: 30%\n",
            "  bucket 2: 40%\n",
            "  bucket 2: 50%\n",
            "  bucket 2: 60%\n",
            "  bucket 2: 70%\n",
            "  bucket 2: 80%\n",
            "  bucket 2: 90%\n",
            "  bucket 2: 100%\n",
            "  Sorting block of length 587461 for bucket 2\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 587462 for bucket 2\n",
            "Getting block 3 of 8\n",
            "  Reserving size (790426) for bucket 3\n",
            "  Calculating Z arrays for bucket 3\n",
            "  Entering block accumulator loop for bucket 3:\n",
            "  bucket 3: 10%\n",
            "  bucket 3: 20%\n",
            "  bucket 3: 30%\n",
            "  bucket 3: 40%\n",
            "  bucket 3: 50%\n",
            "  bucket 3: 60%\n",
            "  bucket 3: 70%\n",
            "  bucket 3: 80%\n",
            "  bucket 3: 90%\n",
            "  bucket 3: 100%\n",
            "  Sorting block of length 383475 for bucket 3\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 383476 for bucket 3\n",
            "Getting block 4 of 8\n",
            "  Reserving size (790426) for bucket 4\n",
            "  Calculating Z arrays for bucket 4\n",
            "  Entering block accumulator loop for bucket 4:\n",
            "  bucket 4: 10%\n",
            "  bucket 4: 20%\n",
            "  bucket 4: 30%\n",
            "  bucket 4: 40%\n",
            "  bucket 4: 50%\n",
            "  bucket 4: 60%\n",
            "  bucket 4: 70%\n",
            "  bucket 4: 80%\n",
            "  bucket 4: 90%\n",
            "  bucket 4: 100%\n",
            "  Sorting block of length 563881 for bucket 4\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 563882 for bucket 4\n",
            "Getting block 5 of 8\n",
            "  Reserving size (790426) for bucket 5\n",
            "  Calculating Z arrays for bucket 5\n",
            "  Entering block accumulator loop for bucket 5:\n",
            "  bucket 5: 10%\n",
            "  bucket 5: 20%\n",
            "  bucket 5: 30%\n",
            "  bucket 5: 40%\n",
            "  bucket 5: 50%\n",
            "  bucket 5: 60%\n",
            "  bucket 5: 70%\n",
            "  bucket 5: 80%\n",
            "  bucket 5: 90%\n",
            "  bucket 5: 100%\n",
            "  Sorting block of length 533109 for bucket 5\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 533110 for bucket 5\n",
            "Getting block 6 of 8\n",
            "  Reserving size (790426) for bucket 6\n",
            "  Calculating Z arrays for bucket 6\n",
            "  Entering block accumulator loop for bucket 6:\n",
            "  bucket 6: 10%\n",
            "  bucket 6: 20%\n",
            "  bucket 6: 30%\n",
            "  bucket 6: 40%\n",
            "  bucket 6: 50%\n",
            "  bucket 6: 60%\n",
            "  bucket 6: 70%\n",
            "  bucket 6: 80%\n",
            "  bucket 6: 90%\n",
            "  bucket 6: 100%\n",
            "  Sorting block of length 781164 for bucket 6\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 781165 for bucket 6\n",
            "Getting block 7 of 8\n",
            "  Reserving size (790426) for bucket 7\n",
            "  Calculating Z arrays for bucket 7\n",
            "  Entering block accumulator loop for bucket 7:\n",
            "  bucket 7: 10%\n",
            "  bucket 7: 20%\n",
            "  bucket 7: 30%\n",
            "  bucket 7: 40%\n",
            "  bucket 7: 50%\n",
            "  bucket 7: 60%\n",
            "  bucket 7: 70%\n",
            "  bucket 7: 80%\n",
            "  bucket 7: 90%\n",
            "  bucket 7: 100%\n",
            "  Sorting block of length 116073 for bucket 7\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:00\n",
            "Returning block of 116074 for bucket 7\n",
            "Getting block 8 of 8\n",
            "  Reserving size (790426) for bucket 8\n",
            "  Calculating Z arrays for bucket 8\n",
            "  Entering block accumulator loop for bucket 8:\n",
            "  bucket 8: 10%\n",
            "  bucket 8: 20%\n",
            "  bucket 8: 30%\n",
            "  bucket 8: 40%\n",
            "  bucket 8: 50%\n",
            "  bucket 8: 60%\n",
            "  bucket 8: 70%\n",
            "  bucket 8: 80%\n",
            "  bucket 8: 90%\n",
            "  bucket 8: 100%\n",
            "  Sorting block of length 746802 for bucket 8\n",
            "  (Using difference cover)\n",
            "  Sorting block time: 00:00:01\n",
            "Returning block of 746803 for bucket 8\n",
            "Exited Ebwt loop\n",
            "fchr[A]: 0\n",
            "fchr[C]: 1188073\n",
            "fchr[G]: 2107357\n",
            "fchr[T]: 3022469\n",
            "fchr[$]: 4215606\n",
            "Exiting Ebwt::buildToDisk()\n",
            "Returning from initFromVector\n",
            "Wrote 5599784 bytes to primary EBWT file: /content/ref_genome/GENOME/GENOME.rev.1.bt2\n",
            "Wrote 1053908 bytes to secondary EBWT file: /content/ref_genome/GENOME/GENOME.rev.2.bt2\n",
            "Re-opening _in1 and _in2 as input streams\n",
            "Returning from Ebwt constructor\n",
            "Headers:\n",
            "    len: 4215606\n",
            "    bwtLen: 4215607\n",
            "    sz: 1053902\n",
            "    bwtSz: 1053902\n",
            "    lineRate: 6\n",
            "    offRate: 4\n",
            "    offMask: 0xfffffff0\n",
            "    ftabChars: 10\n",
            "    eftabLen: 20\n",
            "    eftabSz: 80\n",
            "    ftabLen: 1048577\n",
            "    ftabSz: 4194308\n",
            "    offsLen: 263476\n",
            "    offsSz: 1053904\n",
            "    lineSz: 64\n",
            "    sideSz: 64\n",
            "    sideBwtSz: 48\n",
            "    sideBwtLen: 192\n",
            "    numSides: 21957\n",
            "    numLines: 21957\n",
            "    ebwtTotLen: 1405248\n",
            "    ebwtTotSz: 1405248\n",
            "    color: 0\n",
            "    reverse: 1\n",
            "Total time for backward call to driver() for mirror index: 00:00:03\n",
            "Performing TnSeq analysis on 2413...\n",
            "2413: Mapping with Bowtie2...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Condition analysis"
      ],
      "metadata": {
        "id": "_i02lVmjZRWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/Tn-seek/\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "#@markdown Please upload your condition fastq file, and note the file prefix\n",
        "original_name = next(iter(uploaded.keys()))\n",
        "# New file name (change extension to '.fastq')\n",
        "new_name = original_name.replace('.fq', '.fastq')\n",
        "\n",
        "# Save the uploaded file with the new name\n",
        "with open(new_name, 'wb') as f:\n",
        "    f.write(uploaded[original_name])\n",
        "condition_filename = original_name.replace('.fq', '')\n",
        "prefix2 = condition_filename"
      ],
      "metadata": {
        "id": "tMxTj4s0Y5CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Optionally we will save your fastq file to your google drive if it isnt already, as uploading this takes a long time.\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "filename = f'{condition_filename}.fastq'\n",
        "destination_folder = '/content/drive/My Drive/Tn-seek/'\n",
        "for filename in uploaded.keys():\n",
        "    shutil.copy(filename, destination_folder + filename)\n",
        "    print(f'copied \"{filename}\" to \"{destination_folder}\"')"
      ],
      "metadata": {
        "id": "-bnY7HoIiO9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown This code will find repeat sequences within your fastq and remove them..\n",
        "# Step 1: Read the File and Sample the First 1000 Lines\n",
        "file_path1 = f'{condition_filename}.fastq'\n",
        "sample_size = 10000\n",
        "condition_filename = condition_filename.replace('.fastq', '')\n",
        "prefix2= condition_filename\n",
        "file_path1 = filename\n",
        "print(file_path)\n",
        "lines = []\n",
        "with open(file_path1, 'r') as file:\n",
        "    for i in range(sample_size):\n",
        "        line = file.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        lines.append(line.strip())\n",
        "\n",
        "# Step 2: Extract Nucleotide Sequences\n",
        "nucleotide_sequences = [lines[i] for i in range(1, len(lines), 4)]\n",
        "\n",
        "# Step 3: Count Occurrences of Sequences\n",
        "from collections import defaultdict\n",
        "\n",
        "sequence_counts = defaultdict(int)\n",
        "\n",
        "def count_subsequences(seq, min_length):\n",
        "    for length in range(min_length, len(seq) + 1):\n",
        "        for i in range(len(seq) - length + 1):\n",
        "            subseq = seq[i:i+length]\n",
        "            sequence_counts[subseq] += 1\n",
        "\n",
        "# Count subsequences for all nucleotide sequences\n",
        "for seq in nucleotide_sequences:\n",
        "    count_subsequences(seq, 5)\n",
        "\n",
        "total_sequences = len(nucleotide_sequences)\n",
        "\n",
        "# Step 4: Determine Frequent Sequences\n",
        "frequent_sequences = {\n",
        "    seq: count for seq, count in sequence_counts.items()\n",
        "    if (len(seq) > 15 and count / total_sequences > 0.10) or\n",
        "       (len(seq) > 7 and count / total_sequences > 0.30)\n",
        "}\n",
        "\n",
        "# Output the results\n",
        "frequent_sequences_list = list(frequent_sequences.keys())\n",
        "output_filename = f'IR_sequences_{condition_filename}.txt'\n",
        "with open(output_filename, 'w') as file:\n",
        "    for seq in frequent_sequences_list:\n",
        "        file.write(f\"{seq}\\n\")\n",
        "# Read the sequences from the file into a list\n",
        "with open(output_filename, 'r') as file:\n",
        "    sequences = [line.strip() for line in file]\n",
        "\n",
        "# Sort sequences based on their lengths in descending order\n",
        "sequences_sorted = sorted(sequences, key=len, reverse=True)\n",
        "\n",
        "# Write the sorted sequences back to the file\n",
        "with open(f'IR_sequences_{condition_filename}_sorted.txt', 'w') as file:\n",
        "    for sequence in sequences_sorted:\n",
        "        file.write(sequence + '\\n')\n",
        "print(f'Saved sequences to IR_sequences_{condition_filename}_sorted.txt')\n",
        "\n",
        "output_filename = f'IR_sequences_{condition_filename}_sorted.txt'\n",
        "with open(output_filename, 'r') as file:\n",
        "    sequences = [line.strip() for line in file]\n",
        "\n",
        "# Step 2: Sort sequences based on their lengths in descending order\n",
        "sequences_sorted = sorted(sequences, key=len, reverse=True)\n",
        "\n",
        "# Step 3: Write the sorted sequences to a new file in numbered FASTA format\n",
        "fasta_output_filename1 = f'IR_sequences_{condition_filename}_sorted.fasta'\n",
        "with open(fasta_output_filename1, 'w') as file:\n",
        "    for index, sequence in enumerate(sequences_sorted, start=1):\n",
        "        file.write(f\">Sequence_{index}\\n\")\n",
        "        file.write(f\"{sequence}\\n\")\n",
        "\n",
        "print(f'Saved sequences to {fasta_output_filename1}')"
      ],
      "metadata": {
        "id": "dXqjCVqaVC5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "fasta1 = f'IR_sequences_{condition_filename}_sorted.fasta'\n",
        "# Copy original fastq file to .trim.fastq\n",
        "shutil.copyfile(f'{condition_filename}.fastq', f'{condition_filename}.trim.fastq')\n",
        "fastq_file1 = f'{condition_filename}.fastq'\n",
        "cut_fastq1 = f'{condition_filename}.trim.fastq'\n",
        "# Iterate over sequences and run cutadapt\n",
        "!cutadapt -b file:{fasta1} -m 15 -l 15 -o {cut_fastq1} {fastq_file1}\n",
        "print(\"All sequences processed with cutadapt.\")"
      ],
      "metadata": {
        "id": "qGZckSdOVgcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ir_seq2 = 'GCTA'\n",
        "!./TnSeq3-2.sh -i {ir_seq2} -g /content/ref_genome/GENOME/GENOME {prefix2}"
      ],
      "metadata": {
        "id": "plT_uTCuZHp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tnseq visualisation and comparison"
      ],
      "metadata": {
        "id": "RR279auhZalF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas matplotlib\n"
      ],
      "metadata": {
        "id": "l2OOcBjuPkYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Align your cleaned fq files to the genome to find transposon insertion site frequencies for each gene.\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import shutil\n",
        "import gzip\n",
        "\n",
        "# Function to download the GFF file\n",
        "def download_gff(url, output_path):\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(output_path, 'wb') as out_file:\n",
        "        shutil.copyfileobj(response.raw, out_file)\n",
        "    del response\n",
        "\n",
        "# Function to parse GFF file and extract genes\n",
        "def parse_gff(gff_file):\n",
        "    genes = []\n",
        "    with gzip.open(gff_file, 'rt') as file:\n",
        "        for line in file:\n",
        "            if line.startswith('#'):\n",
        "                continue\n",
        "            parts = line.strip().split('\\t')\n",
        "            if parts[2] == 'gene':\n",
        "                start = int(parts[3])\n",
        "                end = int(parts[4])\n",
        "                strand = parts[6]\n",
        "                attributes = parts[8]\n",
        "                gene_id = attributes.split(';')[0].split('=')[1]\n",
        "                gene_name = attributes.split(';')[1].split('=')[1] if len(attributes.split(';')) > 1 else \"\"\n",
        "                gene_length = end - start + 1  # Calculate gene length\n",
        "                genes.append((start, end, strand, gene_id, gene_name, gene_length))\n",
        "    return pd.DataFrame(genes, columns=['start', 'end', 'strand', 'gene_id', 'gene_name', 'gene_length'])\n",
        "\n",
        "# Function to process transposon sites and find hits\n",
        "def process_transposon_sites(prefix, gff_file):\n",
        "    # Load transposon insertion sites\n",
        "    transposon_sites = pd.read_csv(f'{prefix}/{prefix}-sites.txt', delim_whitespace=True, header=None)\n",
        "    transposon_sites.columns = ['count', 'position']\n",
        "\n",
        "    # Parse GFF file to get genes dataframe\n",
        "    genes_df = parse_gff(gff_file)\n",
        "\n",
        "    results = []\n",
        "    for _, row in transposon_sites.iterrows():\n",
        "        position = row['position']\n",
        "        count = row['count']\n",
        "        hit_genes = genes_df[(genes_df['start'] <= position) & (genes_df['end'] >= position)]\n",
        "        for _, gene in hit_genes.iterrows():\n",
        "            results.append({\n",
        "                \"prefix\": prefix,\n",
        "                \"transposon_position\": position,\n",
        "                \"count\": count,\n",
        "                \"gene_id\": gene['gene_id'],\n",
        "                \"gene_name\": gene['gene_name'],\n",
        "                \"gene_start\": gene['start'],\n",
        "                \"gene_end\": gene['end'],\n",
        "                \"strand\": gene['strand'],\n",
        "                \"gene_length\": gene['gene_length']  # Include gene length\n",
        "            })\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Aggregate counts per gene (summing counts from different positions)\n",
        "    aggregated_results_df = results_df.groupby(['gene_id', 'gene_name', 'gene_length']).agg({\n",
        "        'count': 'sum'\n",
        "    }).reset_index()\n",
        "\n",
        "    return aggregated_results_df\n",
        "\n",
        "# URL for Bacillus subtilis GFF file from Ensembl Bacteria\n",
        "gff_url = \"https://ftp.ensemblgenomes.ebi.ac.uk/pub/bacteria/release-59/gff3/bacteria_0_collection/bacillus_subtilis_subsp_subtilis_str_168_gca_000009045/Bacillus_subtilis_subsp_subtilis_str_168_gca_000009045.ASM904v1.59.gff3.gz\"\n",
        "gff_file = \"Bacillus_subtilis_subsp_subtilis_str_168_gca_000009045.ASM904v1.59.gff3.gz\"\n",
        "\n",
        "# Download the GFF file\n",
        "download_gff(gff_url, gff_file)\n",
        "\n",
        "# Parse the GFF file to get genes dataframe\n",
        "genes_df = parse_gff(gff_file)\n",
        "\n",
        "# Process transposon sites for prefix1 (wildtype)\n",
        "\n",
        "wildtype_results_df = process_transposon_sites(prefix1, gff_file)\n",
        "wildtype_results_df.to_csv(f'{prefix1}_transposon_hits.csv', index=False)\n",
        "\n",
        "# Process transposon sites for prefix2 (condition)\n",
        "\n",
        "condition_results_df = process_transposon_sites(prefix2, gff_file)\n",
        "condition_results_df.to_csv(f'{prefix2}_transposon_hits.csv', index=False)\n",
        "\n",
        "# Ensure all genes are included by merging with the complete gene list\n",
        "wildtype_full_df = pd.merge(genes_df, wildtype_results_df, on=['gene_id', 'gene_name', 'gene_length'], how='left').fillna(0)\n",
        "condition_full_df = pd.merge(genes_df, condition_results_df, on=['gene_id', 'gene_name', 'gene_length'], how='left').fillna(0)\n",
        "\n",
        "# Merge wildtype and condition results on gene_id and gene_name\n",
        "merged_results_df = pd.merge(wildtype_full_df[['gene_id', 'gene_name', 'gene_length', 'count']],\n",
        "                             condition_full_df[['gene_id', 'gene_name', 'gene_length', 'count']],\n",
        "                             on=['gene_id', 'gene_name', 'gene_length'],\n",
        "                             suffixes=('_wildtype', '_condition'),\n",
        "                             how='outer')\n",
        "\n",
        "# Ensure that merged dataframe has zero counts for missing values\n",
        "merged_results_df['count_wildtype'].fillna(0, inplace=True)\n",
        "merged_results_df['count_condition'].fillna(0, inplace=True)\n",
        "\n",
        "# Save merged results to CSV\n",
        "merged_results_df.to_csv('Tnseek-transposon_hits_merged.csv', index=False)\n",
        "\n",
        "# Print the first few rows of the merged results\n",
        "print(merged_results_df.head())\n"
      ],
      "metadata": {
        "id": "Cu3Sppkiw8H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Filter the genes to remove Housekeeping genes and common insertion sites (extremely high and low tn insertion frequencies in the wildtype) and plot a ratio graph\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#@markdown The housekeeping threshold is the number of basepairs between each transposon. A way to reduce the background of housekeeping genes and background noise between samples is to reduce this threshold. This reduces contenders that already have very low counts from the data. Eventually however this will affect all genes, and randomly remove good data.\n",
        "housekeeping_threshold = 200 #@param\n",
        "housekeeping_threshold = 1/housekeeping_threshold\n",
        "# Load the merged results CSV (replace with your actual CSV file path)\n",
        "merged_results_df = pd.read_csv('Tnseek-transposon_hits_merged.csv')\n",
        "\n",
        "# Add TIF (Transposon Insertion Frequency) columns with handling for division by zero\n",
        "merged_results_df['TIF_WT'] = merged_results_df.apply(\n",
        "    lambda row: row['count_wildtype'] / row['gene_length'] if row['gene_length'] != 0 else 0, axis=1\n",
        ")\n",
        "merged_results_df['TIF_Condition'] = merged_results_df.apply(\n",
        "    lambda row: row['count_condition'] / row['gene_length'] if row['gene_length'] != 0 else 0, axis=1\n",
        ")\n",
        "\n",
        "# Add RATIO columns with handling for division by zero\n",
        "merged_results_df['RATIO'] = merged_results_df.apply(\n",
        "    lambda row: row['TIF_WT'] / row['TIF_Condition'] if row['TIF_Condition'] != 0 else np.inf, axis=1\n",
        ")\n",
        "merged_results_df['RATIO_BOTHENDS'] = merged_results_df.apply(\n",
        "    lambda row: max(row['RATIO'], 1 / row['RATIO']) if row['RATIO'] != 0 else np.inf, axis=1\n",
        ")\n",
        "\n",
        "# Filter out housekeeping genes and frequent transposon insertion sites\n",
        "merged_results_df['RATIO_BOTHENDS_HK_FILTER'] = merged_results_df.apply(\n",
        "    lambda row: row['RATIO_BOTHENDS'] if row['TIF_WT'] > housekeeping_threshold else \"Housekeeping\", axis=1\n",
        ")\n",
        "merged_results_df['TNF1'] = merged_results_df.apply(\n",
        "    lambda row: \"TI_Insertion_site\" if row['TIF_WT'] > 5 else row['RATIO_BOTHENDS_HK_FILTER'], axis=1\n",
        ")\n",
        "merged_results_df['TNF2'] = merged_results_df.apply(\n",
        "    lambda row: \"TI_Insertion_site\" if row['TIF_Condition'] > 5 else row['TNF1'], axis=1\n",
        ")\n",
        "merged_results_df.to_csv('Tnseek-transposon_hits_merged_ratio.csv', index=False)\n",
        "# Filter for non-housekeeping, non-transposon insertion site genes\n",
        "filtered_df = merged_results_df[\n",
        "    (merged_results_df['TNF2'] != 'Housekeeping') &\n",
        "    (merged_results_df['TNF2'] != 'TI_Insertion_site')\n",
        "].copy()  # Ensure a copy of the DataFrame to avoid the SettingWithCopyWarning\n",
        "\n",
        "# Convert 'RATIO' column to numeric, coerce non-numeric values to NaN\n",
        "filtered_df['RATIO'] = pd.to_numeric(filtered_df['RATIO'], errors='coerce')\n",
        "\n",
        "# Remove rows with infinite 'RATIO' values\n",
        "filtered_df = filtered_df[~np.isinf(filtered_df['RATIO'])]\n",
        "\n",
        "# Separate plot for the ratio\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(filtered_df['gene_id'], filtered_df['RATIO'], c='black', label='Filtered Genes')\n",
        "\n",
        "# Label top 20 genes based on ratio\n",
        "\n",
        "top_20_genes = filtered_df.nlargest(20, 'RATIO').reset_index(drop=True)\n",
        "for i, row in top_20_genes.iterrows():\n",
        "    plt.text(row['gene_id'], row['RATIO'], f\"{row['gene_name']} ({row['RATIO']:.2f})\", fontsize=6, color='red', ha='center')\n",
        "bottom_20_genes = filtered_df.nsmallest(20, 'RATIO').reset_index(drop=True)\n",
        "for i, row in bottom_20_genes.iterrows():\n",
        "    plt.text(row['gene_id'], row['RATIO'], f\"{row['gene_name']} ({row['RATIO']:.2f})\", fontsize=6, color='blue', ha='center')\n",
        "\n",
        "plt.yscale('log')\n",
        "plt.axhline(y=1, color='r', linestyle='--', label='Ratio = 1')\n",
        "plt.xlabel('Gene ID')\n",
        "plt.ylabel('Ratio (TIF_WT / TIF_Condition)')\n",
        "plt.title('Transposon Insertion Frequency Ratio for Filtered Genes (Excluding Infinite Values)')\n",
        "plt.legend()\n",
        "plt.xticks([])  # Remove x-axis labels\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Q6hgpTLobw1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(top_20_genes)\n",
        "print(bottom_20_genes)"
      ],
      "metadata": {
        "id": "2lbOGP4MfCiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download results\n",
        "import os\n",
        "from google.colab import files\n",
        "import shutil\n",
        "# Specify the folder path and the output zip file name\n",
        "folder_to_zip = f'/content/Tn-seek/Tn-seek/{wildtype_filename}'\n",
        "output_zip_file = f'TnSeek_{wildtype_filename}.zip'\n",
        "shutil.make_archive(output_zip_file.replace('.zip', ''), 'zip', folder_to_zip)\n",
        "folder_to_zip = f'/content/Tn-seek/Tn-seek/{condition_filename}'\n",
        "output_zip_file = f'TnSeek_{condition_filename}.zip'\n",
        "shutil.make_archive(output_zip_file.replace('.zip', ''), 'zip', folder_to_zip)\n",
        "files.download(\"Tnseek_transposon_hits_merged.csv\")\n",
        "files.download(f\"{wildtype_filename}.zip\")\n",
        "files.download(f\"{condition_filename}.zip\")\n",
        "files.download(f\"{wildtype_filename}_transposon_hits.csv\")\n",
        "files.download(f\"{condition_filename}_transposon_hits.csv\")"
      ],
      "metadata": {
        "id": "iQanDH6dA_o_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}